{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split train / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the split only if it has not already been done\n",
    "if not os.path.exists(utils.dfs_path + '/training.pkl') or not os.path.exists(utils.dfs_path + '/validation.pkl'):\n",
    "    # load all train videos (labelled videos)\n",
    "    all_train_videos = utils.get_train_test_video_names()['train']\n",
    "    all_train_labels = pd.read_pickle(utils.labels_path)\n",
    "\n",
    "    # define split\n",
    "    split = 0.8\n",
    "    np.random.seed(69)\n",
    "    train_videos = np.array(all_train_videos)[np.random.choice(len(all_train_videos), int(0.8 * len(all_train_videos)), replace=False)]\n",
    "    validation_videos = np.setdiff1d(all_train_videos, train_videos, assume_unique=False)\n",
    "    train_videos.sort()\n",
    "    validation_videos.sort()\n",
    "\n",
    "    # create two subdataframes for training and validation\n",
    "    training_df = all_train_labels.loc[all_train_labels['videoname'].isin(train_videos)]\n",
    "    validation_df = all_train_labels.loc[all_train_labels['videoname'].isin(validation_videos)]\n",
    "\n",
    "    training_df.to_pickle(utils.dfs_path + '/training.pkl')\n",
    "    validation_df.to_pickle(utils.dfs_path + '/validation.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11183694"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model\n",
    "model = models.resnet18(pretrained=True).to(utils.device)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, utils.num_classes, device = utils.device)\n",
    "model_name = 'resnet18'\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation and normalization for training\n",
    "# just normalization for validation\n",
    "data_transforms = {\n",
    "    'training': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "GAMMA = 0.1\n",
    "STEP_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion is cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "# decay LR by a factor GAMMA every STEP_SIZE epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch datasets\n",
    "datasets = {x: utils.HernitiaDataset(utils.dfs_path + '/' + x + '.pkl', data_transforms[x])  for x in ['training', 'validation']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data loaders\n",
    "dataloaders = {x: utils.DataLoader(dataset=datasets[x], batch_size=BATCH_SIZE, shuffle=True) for x in ['training', 'validation']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "  5120/164841: [>...............................] - ETA 3219.1s"
     ]
    }
   ],
   "source": [
    "utils.train_model(model = model, \n",
    "                    model_name = model_name,  #  name of the model which will be the name of the saved weights file within /weights\n",
    "                    dataloaders = dataloaders, \n",
    "                    criterion = criterion, \n",
    "                    optimizer = optimizer, \n",
    "                    scheduler = exp_lr_scheduler, \n",
    "                    num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make Kaggle prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testing dataframe already in storage'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and save testing df\n",
    "utils.save_testing_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild model\n",
    "resnet18 = models.resnet18(pretrained=True).to(utils.device)\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, utils.num_classes, device = utils.device)\n",
    "\n",
    "# data augmentation and normalization for training\n",
    "testing_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184003/184003: [===============================>] - ETA 0.4sss\n"
     ]
    }
   ],
   "source": [
    "utils.predict_kaggle(model = resnet18, \n",
    "                    model_name = model_name, # name of the model from which to load the weights within weights/\n",
    "                    transforms = testing_transforms, \n",
    "                    predictions_name = model_name) # name of the csv file to which the predictions are saved within predictions/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
