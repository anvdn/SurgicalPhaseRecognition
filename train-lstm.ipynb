{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import utils\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split train / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the split only if it has not already been done\n",
    "if not os.path.exists(utils.dfs_path + '/training_finetuning.pkl') or not os.path.exists(utils.dfs_path + '/validation_finetuning.pkl'):\n",
    "    # load all train videos (labelled videos)\n",
    "    all_train_videos = utils.get_train_test_video_names()['train']\n",
    "    all_train_labels = pd.read_pickle(utils.labels_path)\n",
    "\n",
    "    # define split\n",
    "    split = 0.8\n",
    "    np.random.seed(69)\n",
    "    train_videos = np.array(all_train_videos)[np.random.choice(len(all_train_videos), int(0.8 * len(all_train_videos)), replace=False)]\n",
    "    validation_videos = np.setdiff1d(all_train_videos, train_videos, assume_unique=False)\n",
    "    train_videos.sort()\n",
    "    validation_videos.sort()\n",
    "\n",
    "    # create two subdataframes for training and validation\n",
    "    training_df = all_train_labels.loc[all_train_labels['videoname'].isin(train_videos)]\n",
    "    validation_df = all_train_labels.loc[all_train_labels['videoname'].isin(validation_videos)]\n",
    "\n",
    "    training_df.to_pickle(utils.dfs_path + '/training_finetuning.pkl')\n",
    "    validation_df.to_pickle(utils.dfs_path + '/validation_finetuning.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.HernitiaModel(model_name = 'mobilenet_v2_lstm', num_classes = utils.num_classes, pretrained = True, skip_lstm = True).to(utils.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2374236"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN (bottleneck) finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation and normalization for training\n",
    "# just normalization for validation\n",
    "data_transforms = {\n",
    "    'training': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "LEARNING_RATE = 0.003\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "GAMMA = 0.3\n",
    "STEP_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion is cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "# decay LR by a factor GAMMA every STEP_SIZE epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch datasets\n",
    "datasets = {x: utils.HernitiaDataset(utils.dfs_path + '/' + x + '_finetuning.pkl', data_transforms[x])  for x in ['training', 'validation']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data loaders\n",
    "dataloaders = {x: utils.DataLoader(dataset=datasets[x], batch_size=BATCH_SIZE, shuffle=True) for x in ['training', 'validation']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n",
      "164841/164841: [===============================>] - ETA 2.9ssss\n",
      "training Loss: 0.4329 Acc: 0.8550\n",
      "50216/50216: [===============================>] - ETA 1.2sss\n",
      "validation Loss: 0.6776 Acc: 0.7934\n",
      "Epoch 2/3\n",
      "----------\n",
      "164841/164841: [===============================>] - ETA 0.8sss\n",
      "training Loss: 0.1807 Acc: 0.9392\n",
      "50216/50216: [===============================>] - ETA 0.4sss\n",
      "validation Loss: 0.6792 Acc: 0.8077\n",
      "Epoch 3/3\n",
      "----------\n",
      "164841/164841: [===============================>] - ETA 0.8sss\n",
      "training Loss: 0.1360 Acc: 0.9552\n",
      "50216/50216: [===============================>] - ETA 0.4sss\n",
      "validation Loss: 0.7272 Acc: 0.8040\n",
      "Training complete in 69m 1s\n",
      "Best val Acc: 0.807691\n"
     ]
    }
   ],
   "source": [
    "utils.train_model(model = model, \n",
    "                    model_name = model.model_name,  #  name of the model which will be the name of the saved weights file within /weights\n",
    "                    dataloaders = dataloaders, \n",
    "                    criterion = criterion, \n",
    "                    optimizer = optimizer, \n",
    "                    scheduler = exp_lr_scheduler, \n",
    "                    num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild model\n",
    "model = model.HernitiaModel(model_name = 'mobilenet_v2_lstm', num_classes = utils.num_classes, pretrained = True, skip_lstm = True).to(utils.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload weights from finetuning\n",
    "model.load_state_dict(torch.load(utils.weights_path + '/' + model.model_name + '.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pad with blank images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process consecutive frames by batch using our dataloaders, each batch loaded should only be composed of frames of a single video. A trick consists of padding each video with blank images at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "if not os.path.exists(utils.dfs_path + '/training_lstm.pkl') or not os.path.exists(utils.dfs_path + '/validation_lstm.pkl'):\n",
    "    for phase in ['training', 'validation']:\n",
    "        df = pd.read_pickle(utils.dfs_path + '/' + phase + '_finetuning.pkl')\n",
    "        for videoname in df['videoname'].unique():\n",
    "            # pad with blank images at the end of each video\n",
    "            # so as to only have number of frames multiple of BATCH_SIZE\n",
    "            # we can then process through the LSTM by batch without shuffling\n",
    "            num_frames = df[df.videoname == videoname].shape[0]\n",
    "            num_rows_to_add = (BATCH_SIZE - (num_frames % BATCH_SIZE)) % BATCH_SIZE\n",
    "            template_white_row = {'videoname': videoname, 'frame': 10000, 'label': -1}\n",
    "            white_rows_to_add = pd.DataFrame([template_white_row for _ in range(num_rows_to_add)])\n",
    "            df = pd.concat([df, white_rows_to_add], ignore_index=True)\n",
    "        df['sort'] = df['videoname'].str[-3:].astype(int)\n",
    "        # sort rows\n",
    "        df.sort_values(['sort', 'frame'],inplace=True, ascending=True)\n",
    "        df = df.drop('sort', axis=1)\n",
    "        # save df\n",
    "        df.to_pickle(utils.dfs_path + '/' + phase + '_lstm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Set training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reincorporate lstm and freeze bottleneck\n",
    "model.skip_lstm = False\n",
    "model.freeze_bottleneck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "LEARNING_RATE = 0.003\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "MOMENTUM = 0.9\n",
    "GAMMA = 0.3\n",
    "STEP_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion is cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "# decay LR by a factor GAMMA every STEP_SIZE epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch datasets\n",
    "datasets = {x: utils.HernitiaDataset(utils.dfs_path + '/' + x + '_lstm.pkl', data_transforms[x])  for x in ['training', 'validation']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data loaders\n",
    "dataloaders = {x: utils.DataLoader(dataset=datasets[x], batch_size=BATCH_SIZE, shuffle=False) for x in ['training', 'validation']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils.train_model(model = model, \n",
    "                    model_name = model.model_name,  #  name of the model which will be the name of the saved weights file within /weights\n",
    "                    dataloaders = dataloaders, \n",
    "                    criterion = criterion, \n",
    "                    optimizer = optimizer, \n",
    "                    scheduler = exp_lr_scheduler, \n",
    "                    num_epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
