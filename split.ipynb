{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess the accuracy of all our models, we create and save different dataframes corresponding to training, validation and testing phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell MUST be run\n",
    "# set a numpy seed to keep the same dataframes across machines\n",
    "np.random.seed(69)\n",
    "\n",
    "# set split training / validation\n",
    "split = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split training / validation without temporal organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use these dataframes in the case where our model does not implement correlation within elements of a same batch. This means that at training time, we load a complete random batch of frames. We also perform oversampling so as to have complete class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the split only if it has not already been done\n",
    "if not os.path.exists(utils.dfs_path + '/training_no_temp.pkl') or not os.path.exists(utils.dfs_path + '/validation_no_temp.pkl') or not os.path.exists(utils.dfs_path + '/training_no_temp_os.pkl') or os.path.exists(utils.dfs_path + '/validation_no_temp_os.pkl'):\n",
    "    # load all train videos (labelled videos)\n",
    "    all_train_videos = utils.get_train_test_video_names()['train']\n",
    "    all_train_labels = pd.read_pickle(utils.labels_path)\n",
    "\n",
    "    train_videos = np.array(all_train_videos)[np.random.choice(len(all_train_videos), int(split * len(all_train_videos)), replace=False)]\n",
    "    validation_videos = np.setdiff1d(all_train_videos, train_videos, assume_unique=False)\n",
    "    train_videos.sort()\n",
    "    validation_videos.sort()\n",
    "\n",
    "    # create two subdataframes for training and validation\n",
    "    training_df = all_train_labels.loc[all_train_labels['videoname'].isin(train_videos)]\n",
    "    validation_df = all_train_labels.loc[all_train_labels['videoname'].isin(validation_videos)]\n",
    "\n",
    "    # add video number of frames as feature\n",
    "    for phase in ['training', 'validation']:\n",
    "        df = training_df.copy() if phase == 'training' else validation_df.copy()\n",
    "        df['video_num_frames'] = (df.groupby('videoname')['frame'].transform('max') + 1).astype(int)\n",
    "        df.to_pickle(utils.dfs_path + '/' + phase + '_no_temp.pkl')\n",
    "\n",
    "        # oversample df\n",
    "        df_size = df['label'].value_counts().max()\n",
    "        lst = [df]\n",
    "        for class_index, group in df.groupby('label'):\n",
    "            lst.append(group.sample(df_size-len(group), replace=True))\n",
    "        df = pd.concat(lst)\n",
    "        df.to_pickle(utils.dfs_path + '/' + phase + '_no_temp_os.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split training / validation with temporal organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use these dataframes in the case where our model does implement correlation within elements of a same batch. In other words, we want each batch to contain frames of the same video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "if not os.path.exists(utils.dfs_path + '/training_temp.pkl') or not os.path.exists(utils.dfs_path + '/validation_temp.pkl'):\n",
    "    for phase in ['training', 'validation']:\n",
    "        df = pd.read_pickle(utils.dfs_path + '/' + phase + '_no_temp.pkl')\n",
    "        for videoname in df['videoname'].unique():\n",
    "            # pad with black images at the end of each video\n",
    "            # so as to only have number of frames multiple of BATCH_SIZE\n",
    "            # we can then process through the LSTM by batch without shuffling\n",
    "            num_frames = df[df.videoname == videoname].shape[0]\n",
    "            num_rows_to_add = (BATCH_SIZE - (num_frames % BATCH_SIZE)) % BATCH_SIZE\n",
    "            video_num_frames = df[df.videoname == videoname]['video_num_frames'].tolist()[0]\n",
    "            template_white_row = {'videoname': videoname, 'frame': 10000, 'label': -1, 'video_num_frames': video_num_frames}\n",
    "            white_rows_to_add = pd.DataFrame([template_white_row for _ in range(num_rows_to_add)])\n",
    "            df = pd.concat([df, white_rows_to_add], ignore_index=True)\n",
    "        df['sort'] = df['videoname'].str[-12].astype(int) * 10000 + df['videoname'].str[-3:].astype(int)\n",
    "        # sort rows\n",
    "        df.sort_values(['sort', 'frame'],inplace=True, ascending=True)\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.drop(['sort', 'index'], axis=1)\n",
    "        # shuffle batches\n",
    "        index_list = np.array(df.index)\n",
    "        np.random.shuffle(np.reshape(index_list, (-1, BATCH_SIZE)))\n",
    "        shuffled_df = df.loc[index_list, :]\n",
    "        shuffled_df.reset_index(inplace=True)\n",
    "        shuffled_df = shuffled_df.drop(['index'], axis=1)\n",
    "        # save df\n",
    "        shuffled_df.to_pickle(utils.dfs_path + '/' + phase + '_temp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save testing df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to produce the testing df. As the weights are not updated at test time, we are able to use exactly the same format as the temporal training / validation dataframes. This way we can test all models on this testing df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(utils.dfs_path + '/testing.pkl'):\n",
    "    # list names of all videos in the test set\n",
    "    surgeon1_test_videonames = ['RALIHR_surgeon01_fps01_' + str(x).zfill(4) for x in range(71,126)]\n",
    "    surgeon2_test_videonames = ['RALIHR_surgeon02_fps01_' + str(x).zfill(4) for x in range(1,5)]\n",
    "    surgeon3_test_videonames = ['RALIHR_surgeon03_fps01_0001']\n",
    "    all_test_videonames = surgeon1_test_videonames + surgeon2_test_videonames + surgeon3_test_videonames\n",
    "\n",
    "    # generate df with all frames of these videos\n",
    "    videonames = []\n",
    "    frames = []\n",
    "    Ids = [] # id list for kaggle prediction\n",
    "    for videoname in all_test_videonames:\n",
    "        video_id = re.sub('[^0-9]', '',  videoname)\n",
    "        video_id = video_id[0:2].zfill(3) + '-' + video_id[-4:] + '-'\n",
    "        for frame in range(utils.count_frames(videoname)):\n",
    "            videonames.append(videoname)\n",
    "            frames.append(frame)\n",
    "            Ids.append(video_id + str(frame + 1).zfill(5))\n",
    "    df = pd.DataFrame({'videoname' : videonames, 'frame' : frames, 'Id': Ids})\n",
    "    # add number of frames per video as feature\n",
    "    df['video_num_frames'] = (df.groupby('videoname')['frame'].transform('max') + 1).astype(int)\n",
    "\n",
    "    for videoname in df['videoname'].unique():\n",
    "        # pad with black images at the end of each video\n",
    "        # so as to only have number of frames multiple of BATCH_SIZE\n",
    "        # we can then process through the LSTM by batch without shuffling\n",
    "        num_frames = df[df.videoname == videoname].shape[0]\n",
    "        num_rows_to_add = (BATCH_SIZE - (num_frames % BATCH_SIZE)) % BATCH_SIZE\n",
    "        video_num_frames = df[df.videoname == videoname]['video_num_frames'].tolist()[0]\n",
    "        Id = 'fake'\n",
    "        template_white_row = {'videoname': videoname, 'frame': 10000, 'video_num_frames': video_num_frames, 'Id': Id}\n",
    "        white_rows_to_add = pd.DataFrame([template_white_row for _ in range(num_rows_to_add)])\n",
    "        df = pd.concat([df, white_rows_to_add], ignore_index=True)\n",
    "    df['sort'] = df['videoname'].str[-12].astype(int) * 10000 + df['videoname'].str[-3:].astype(int)\n",
    "    # sort rows\n",
    "    df.sort_values(['sort', 'frame'],inplace=True, ascending=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.drop(['sort', 'index'], axis=1)\n",
    "    # shuffle batches\n",
    "    index_list = np.array(df.index)\n",
    "    np.random.shuffle(np.reshape(index_list, (-1, BATCH_SIZE)))\n",
    "    shuffled_df = df.loc[index_list, :]\n",
    "    shuffled_df.reset_index(inplace=True)\n",
    "    shuffled_df = shuffled_df.drop(['index'], axis=1)\n",
    "    # save df\n",
    "    shuffled_df.to_pickle(utils.dfs_path + '/testing.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create master training dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate training and validation dfs with temporal organization to obtain a master training df that we can use to train all our models (modeling temporal correlation or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(utils.dfs_path + '/master_training_temp.pkl') or not os.path.exists(utils.dfs_path + '/master_training_no_temp.pkl'):\n",
    "    for mode in ['temp', 'no_temp']:\n",
    "        training_df = pd.read_pickle(utils.dfs_path + '/training_' + mode + '.pkl')\n",
    "        validation_df = pd.read_pickle(utils.dfs_path + '/validation_' + mode + '.pkl')\n",
    "        master_training_df = pd.concat([training_df, validation_df])\n",
    "        master_training_df.to_pickle(utils.dfs_path + '/master_training_' + mode + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
